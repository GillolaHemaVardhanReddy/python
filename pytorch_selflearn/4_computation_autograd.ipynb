{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Step 1E ‚Äî Computation Graph & Autograd (The Heart of Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know how to manipulate tensors ‚Äî but tensors alone can‚Äôt ‚Äúlearn.‚Äù\n",
    "What gives PyTorch its magic is autograd and the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 1Ô∏è‚É£ What Is a Computation Graph?\n",
    "\n",
    "Whenever you perform mathematical operations on tensors in PyTorch,\n",
    "it secretly builds a graph of operations.\n",
    "\n",
    "* The nodes of this graph are tensors (values).\n",
    "\n",
    "* The edges are operations (like addition, multiplication, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you do:\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * 3\n",
    "z = y + 5\n",
    "\n",
    "\n",
    "PyTorch internally builds:\n",
    "\n",
    "x ‚îÄ‚îÄ(√ó3)‚îÄ‚îÄ‚ñ∫ y ‚îÄ‚îÄ(+5)‚îÄ‚îÄ‚ñ∫ z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßÆ 2Ô∏è‚É£ What Happens During .backward()\n",
    "\n",
    "When you call z.backward(), PyTorch walks backward through that graph,\n",
    "applying the chain rule of calculus to compute the derivative\n",
    "of z with respect to every tensor that had requires_grad=True.\n",
    "\n",
    "So here:\n",
    "\n",
    "* dz/dx = 3\n",
    "\n",
    "* That value gets stored in x.grad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3Ô∏è‚É£ The ‚Äúrequires_grad‚Äù Flag\n",
    "\n",
    "This flag tells PyTorch:\n",
    "\n",
    "‚ÄúTrack everything that happens to this tensor so you can compute gradients later.‚Äù\n",
    "\n",
    "If it‚Äôs False (the default), PyTorch ignores that tensor during .backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * 3\n",
    "z = y + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.0, requires_grad=True)  # tracked\n",
    "b = torch.tensor(4.0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you mix them, autograd still works, but only variables with requires_grad=True\n",
    "will receive .grad values.\n",
    "\n",
    "### üö¶ 4Ô∏è‚É£ Why We Need Gradients\n",
    "\n",
    "Gradients tell us the direction and magnitude to adjust parameters  \n",
    "to minimize a loss (error).  \n",
    "\n",
    "Example intuition:  \n",
    "\n",
    "If loss decreases when weight w decreases, the gradient for w is positive ‚Üí  \n",
    "\n",
    "so optimizer subtracts a small step from w.  \n",
    "\n",
    "\n",
    "### üß† 5Ô∏è‚É£ Example: One-variable computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 3 + 2 * x + 1\n",
    "y.backward()          # compute dy/dx\n",
    "print(x.grad)         # ‚Üí 3x¬≤ + 2 = 3*4 + 2 = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.)\n"
     ]
    }
   ],
   "source": [
    "y = x ** 3 + 2 * x + 1\n",
    "y.backward()          # compute dy/dx\n",
    "print(x.grad)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ 6Ô∏è‚É£ What if you call backward multiple times?\n",
    "\n",
    "Gradients accumulate by default ‚Äî they don‚Äôt reset automatically.  \n",
    "\n",
    "Otherwise, you‚Äôll see .grad keep increasing over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()   # always reset before next backward\n",
    "\n",
    "# OR\n",
    "\n",
    "x = x.detach()\n",
    "\n",
    "# Both stop gradient tracking temporarily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 7Ô∏è‚É£ Disabling Autograd (for evaluation/inference)\n",
    "\n",
    "When you‚Äôre not training (for example, testing a model),\n",
    "you don‚Äôt need gradient tracking ‚Äî it saves memory & computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use either:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(x)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# or\u001b[39;00m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Use either:\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(x)\n",
    "\n",
    "\n",
    "# or\n",
    "\n",
    "x = x.detach()\n",
    "\n",
    "\n",
    "# Both stop gradient tracking temporarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 8Ô∏è‚É£ Behind the Scenes (a tiny mental model)\n",
    "\n",
    "Each tensor that participates in autograd has:\n",
    "\n",
    "tensor.data ‚Üí its raw value\n",
    "\n",
    "tensor.grad ‚Üí its computed gradient (after backward)\n",
    "\n",
    "tensor.grad_fn ‚Üí a reference to the operation that created it\n",
    "\n",
    "Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x7eb7b1c21000>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x * 2\n",
    "print(y.grad_fn)   # prints something like <MulBackward0 object>\n",
    "\n",
    "# That‚Äôs literally PyTorch keeping a ‚Äúrecipe‚Äù to reverse the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 9Ô∏è‚É£ Mini Exercises (try these slowly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Create x = torch.tensor(2.0, requires_grad=True)\n",
    "Compute y = 3*x**2 + 4*x + 5\n",
    "Find x.grad after .backward() ‚Äî verify manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Repeat but call .backward() twice without zeroing grad.\n",
    "What happens to x.grad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 3*x**2 + 4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Wrap your computation in with torch.no_grad(): and check ‚Äî\n",
    "does x.grad update anymore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß© 3Ô∏è‚É£ Perfect mental model üí≠\n",
    "\n",
    "You can think of autograd as a notebook keeping track of every math step you do.\n",
    "When you call backward(), PyTorch flips through the notebook in reverse, applying the chain rule.\n",
    "\n",
    "But when you use torch.no_grad(), you‚Äôre saying:\n",
    "\n",
    "‚ÄúDon‚Äôt write anything in the notebook for a while.‚Äù\n",
    "\n",
    "So afterward, when you ask it to differentiate, there‚Äôs literally no record of the steps to go back through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚öôÔ∏è 4Ô∏è‚É£ Why we still use torch.no_grad()\n",
    "\n",
    "Even though it blocks gradient tracking, it‚Äôs very useful when you‚Äôre:\n",
    "\n",
    "evaluating / testing models\n",
    "\n",
    "generating outputs (no training)\n",
    "\n",
    "saving memory (since graph tracking uses extra memory)\n",
    "\n",
    "preventing accidental gradient updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# has no effect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mequal(x\u001b[38;5;241m.\u001b[39mgrad, before))  \u001b[38;5;66;03m# should be True\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/python/pythonPractiveEnv/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/python/pythonPractiveEnv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/python/pythonPractiveEnv/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "before = x.grad.clone()\n",
    "with torch.no_grad():\n",
    "    y = 3*x**2 + 4*x + 5\n",
    "    y.backward()    # has no effect\n",
    "print(torch.equal(x.grad, before))  # should be True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### causes the error because:\n",
    "\n",
    "with torch.no_grad(): tells PyTorch:\n",
    "\n",
    "‚ÄúDo not build the computation graph for any operation inside this block.‚Äù\n",
    "\n",
    "So when you do  \n",
    "\n",
    "y = 3*x**2 + 4*x + 5,  \n",
    "\n",
    "that y does not know it came from x.  \n",
    "\n",
    "It‚Äôs just a plain tensor, not connected to x.grad_fn anymore.\n",
    "\n",
    "Hence, when you call .backward(), PyTorch says:\n",
    "\n",
    "‚ÄúSorry, this tensor isn‚Äôt part of a computation graph ‚Äî there‚Äôs nothing to differentiate!‚Äù\n",
    "\n",
    "That‚Äôs literally what\n",
    "\n",
    "‚Äúdoes not require grad and does not have a grad_fn‚Äù\n",
    "means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so what you are telling me is in no grad function we won't be storing the graph of operations so the backward differentiation can't be done since it doesn't or can't get the prev operation that is done on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are telling PyTorch:\n",
    "\n",
    "‚ÄúHey, for the operations inside this block, don‚Äôt bother recording how the result was computed.\n",
    "Just give me the output values, I don‚Äôt need gradients later.‚Äù\n",
    "\n",
    "So PyTorch skips building the computation graph ‚Äî\n",
    "no grad_fn objects are attached, no connections are made, nothing to trace later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Create a = torch.tensor([2.0,3.0], requires_grad=True)\n",
    "Compute b = (a**2).sum()  \n",
    "\n",
    "Find a.grad. (Hint: gradient will be [2a‚ÇÅ, 2a‚ÇÇ].)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (a**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† `.backward()` and Autograd ‚Äî How They Relate\n",
    "\n",
    "‚úÖ **Yes.**\n",
    "`autograd` is the **system**, and `.backward()` is the **action** that triggers it.\n",
    "\n",
    "Let‚Äôs think of it like this üëá\n",
    "\n",
    "| Analogy            | Meaning                                                                                                                 |\n",
    "| ------------------ | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| üß© **Autograd**    | The **engine** that records operations and knows how to compute gradients (it builds and stores the computation graph). |\n",
    "| ‚öôÔ∏è **.backward()** | The **command** that tells the autograd engine: ‚ÄúOK, now walk this graph in reverse and compute the derivatives.‚Äù       |\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "# Autograd system is *watching* everything\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3*x + 1\n",
    "\n",
    "# This line tells the autograd engine to start differentiation\n",
    "y.backward()\n",
    "```\n",
    "\n",
    "### Step by step:\n",
    "\n",
    "1Ô∏è‚É£ `requires_grad=True`\n",
    "‚Üí Autograd starts **recording** all ops done on `x`.\n",
    "\n",
    "2Ô∏è‚É£ When you compute `y = ...`,\n",
    "‚Üí Autograd builds a **computation graph** connecting `x ‚Üí y`.\n",
    "\n",
    "3Ô∏è‚É£ When you call `y.backward()`,\n",
    "‚Üí Autograd **traverses the graph backward**, applying the **chain rule** to compute `dy/dx`.\n",
    "\n",
    "4Ô∏è‚É£ The result is stored in\n",
    "\n",
    "```python\n",
    "x.grad\n",
    "```\n",
    "\n",
    "‚úÖ So `.backward()` **uses autograd** internally.\n",
    "Autograd ‚âà the engine; `.backward()` ‚âà pressing the ‚Äúdifferentiate now‚Äù button.\n",
    "\n",
    "---\n",
    "\n",
    "# üîç Example showing autograd & backward in action\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = 2*x**3 + 4*x + 1     # autograd tracks this computation\n",
    "print(y.grad_fn)         # shows grad function, means it's tracked\n",
    "\n",
    "y.backward()             # backward() calls autograd internally\n",
    "print(x.grad)            # gradient: dy/dx = 6x^2 + 4 = 6*9 + 4 = 58\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* `grad_fn` shows that `y` has a recorded operation (proof autograd is tracking).\n",
    "* `y.backward()` triggers autograd‚Äôs engine to compute the gradient.\n",
    "* `.grad` holds the computed result.\n",
    "\n",
    "---\n",
    "\n",
    "# üí° Summary (one-line mental map)\n",
    "\n",
    "> **Autograd = system that builds the graph.**\n",
    "> **.backward() = trigger that makes autograd compute all gradients.**\n",
    "\n",
    "---\n",
    "\n",
    "You‚Äôve now fully connected **tensors ‚Üí computation graph ‚Üí autograd ‚Üí backward() ‚Üí gradients** ‚úÖ\n",
    "That‚Äôs the complete conceptual chain of ‚Äúhow neural networks actually learn.‚Äù\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonPractiveEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
